{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2WR-yMIpcbm"
      },
      "source": [
        "## Important instruction\n",
        "\n",
        "For programming exercises that only editing, only edit the code as shown in the following format.\n",
        "\n",
        "```\n",
        "##############################################\n",
        "\n",
        "#Edit the following code\n",
        "\n",
        "var1 = 3\n",
        "var2 = 4\n",
        "print(var1 + var4)\n",
        "\n",
        "##############################################\n",
        "```\n",
        "\n",
        "You are open to experimenting with the other parts of code but you will only be awarded points if the question asked is answered which only needs finishing or making changes to the code in the above specified format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUm8wagILU0-"
      },
      "source": [
        "## Question 7: Visualising Classification Decision Trees\n",
        "\n",
        "\n",
        "The objective of this question is to implement a Binary Classification Decision Tree with visualizations. Follow the given instructions below:\n",
        "\n",
        "#### Step 1: Import Required Libraries\n",
        "\n",
        "#### Step 2: Load and Preprocess the Data\n",
        "- Load the Iris dataset using `load_iris()`.\n",
        "- Convert the target variable to represent a binary classification: 1 for 'Iris-setosa' and 0 for others.\n",
        "\n",
        "#### Step 3: Split the Dataset\n",
        "- Divide the dataset into training and testing sets using `train_test_split()`, with an 80-20 split and a random state of 99.\n",
        "\n",
        "#### Step 4: Initialize and Train the Classifier\n",
        "- Initialize the `DecisionTreeClassifier` with a random state of 99.\n",
        "- Train the classifier on the training data.\n",
        "\n",
        "#### Step 5: Visualize the Decision Tree\n",
        "- Create a basic visualization of the decision tree. As shown below.\n",
        "\n",
        "![plot1](https://raw.githubusercontent.com/KrishnaTejaJ/datasets-CSCI-B455/main/imagedata/a6q9-1.png)\n",
        "\n",
        "<!-- - Create a detailed visualization of the decision tree on the second subplot as shown below with all the lables and styles.\n",
        "\n",
        "![plot2](https://raw.githubusercontent.com/KrishnaTejaJ/datasets-CSCI-B455/main/imagedata/a6q9-2.png) -->\n",
        "\n",
        "\n",
        "For more information, refer to the following resources:\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
        "- https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uT2Y1fAYAq1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
            "       [4.9, 3. , 1.4, 0.2],\n",
            "       [4.7, 3.2, 1.3, 0.2],\n",
            "       [4.6, 3.1, 1.5, 0.2],\n",
            "       [5. , 3.6, 1.4, 0.2],\n",
            "       [5.4, 3.9, 1.7, 0.4],\n",
            "       [4.6, 3.4, 1.4, 0.3],\n",
            "       [5. , 3.4, 1.5, 0.2],\n",
            "       [4.4, 2.9, 1.4, 0.2],\n",
            "       [4.9, 3.1, 1.5, 0.1],\n",
            "       [5.4, 3.7, 1.5, 0.2],\n",
            "       [4.8, 3.4, 1.6, 0.2],\n",
            "       [4.8, 3. , 1.4, 0.1],\n",
            "       [4.3, 3. , 1.1, 0.1],\n",
            "       [5.8, 4. , 1.2, 0.2],\n",
            "       [5.7, 4.4, 1.5, 0.4],\n",
            "       [5.4, 3.9, 1.3, 0.4],\n",
            "       [5.1, 3.5, 1.4, 0.3],\n",
            "       [5.7, 3.8, 1.7, 0.3],\n",
            "       [5.1, 3.8, 1.5, 0.3],\n",
            "       [5.4, 3.4, 1.7, 0.2],\n",
            "       [5.1, 3.7, 1.5, 0.4],\n",
            "       [4.6, 3.6, 1. , 0.2],\n",
            "       [5.1, 3.3, 1.7, 0.5],\n",
            "       [4.8, 3.4, 1.9, 0.2],\n",
            "       [5. , 3. , 1.6, 0.2],\n",
            "       [5. , 3.4, 1.6, 0.4],\n",
            "       [5.2, 3.5, 1.5, 0.2],\n",
            "       [5.2, 3.4, 1.4, 0.2],\n",
            "       [4.7, 3.2, 1.6, 0.2],\n",
            "       [4.8, 3.1, 1.6, 0.2],\n",
            "       [5.4, 3.4, 1.5, 0.4],\n",
            "       [5.2, 4.1, 1.5, 0.1],\n",
            "       [5.5, 4.2, 1.4, 0.2],\n",
            "       [4.9, 3.1, 1.5, 0.2],\n",
            "       [5. , 3.2, 1.2, 0.2],\n",
            "       [5.5, 3.5, 1.3, 0.2],\n",
            "       [4.9, 3.6, 1.4, 0.1],\n",
            "       [4.4, 3. , 1.3, 0.2],\n",
            "       [5.1, 3.4, 1.5, 0.2],\n",
            "       [5. , 3.5, 1.3, 0.3],\n",
            "       [4.5, 2.3, 1.3, 0.3],\n",
            "       [4.4, 3.2, 1.3, 0.2],\n",
            "       [5. , 3.5, 1.6, 0.6],\n",
            "       [5.1, 3.8, 1.9, 0.4],\n",
            "       [4.8, 3. , 1.4, 0.3],\n",
            "       [5.1, 3.8, 1.6, 0.2],\n",
            "       [4.6, 3.2, 1.4, 0.2],\n",
            "       [5.3, 3.7, 1.5, 0.2],\n",
            "       [5. , 3.3, 1.4, 0.2],\n",
            "       [7. , 3.2, 4.7, 1.4],\n",
            "       [6.4, 3.2, 4.5, 1.5],\n",
            "       [6.9, 3.1, 4.9, 1.5],\n",
            "       [5.5, 2.3, 4. , 1.3],\n",
            "       [6.5, 2.8, 4.6, 1.5],\n",
            "       [5.7, 2.8, 4.5, 1.3],\n",
            "       [6.3, 3.3, 4.7, 1.6],\n",
            "       [4.9, 2.4, 3.3, 1. ],\n",
            "       [6.6, 2.9, 4.6, 1.3],\n",
            "       [5.2, 2.7, 3.9, 1.4],\n",
            "       [5. , 2. , 3.5, 1. ],\n",
            "       [5.9, 3. , 4.2, 1.5],\n",
            "       [6. , 2.2, 4. , 1. ],\n",
            "       [6.1, 2.9, 4.7, 1.4],\n",
            "       [5.6, 2.9, 3.6, 1.3],\n",
            "       [6.7, 3.1, 4.4, 1.4],\n",
            "       [5.6, 3. , 4.5, 1.5],\n",
            "       [5.8, 2.7, 4.1, 1. ],\n",
            "       [6.2, 2.2, 4.5, 1.5],\n",
            "       [5.6, 2.5, 3.9, 1.1],\n",
            "       [5.9, 3.2, 4.8, 1.8],\n",
            "       [6.1, 2.8, 4. , 1.3],\n",
            "       [6.3, 2.5, 4.9, 1.5],\n",
            "       [6.1, 2.8, 4.7, 1.2],\n",
            "       [6.4, 2.9, 4.3, 1.3],\n",
            "       [6.6, 3. , 4.4, 1.4],\n",
            "       [6.8, 2.8, 4.8, 1.4],\n",
            "       [6.7, 3. , 5. , 1.7],\n",
            "       [6. , 2.9, 4.5, 1.5],\n",
            "       [5.7, 2.6, 3.5, 1. ],\n",
            "       [5.5, 2.4, 3.8, 1.1],\n",
            "       [5.5, 2.4, 3.7, 1. ],\n",
            "       [5.8, 2.7, 3.9, 1.2],\n",
            "       [6. , 2.7, 5.1, 1.6],\n",
            "       [5.4, 3. , 4.5, 1.5],\n",
            "       [6. , 3.4, 4.5, 1.6],\n",
            "       [6.7, 3.1, 4.7, 1.5],\n",
            "       [6.3, 2.3, 4.4, 1.3],\n",
            "       [5.6, 3. , 4.1, 1.3],\n",
            "       [5.5, 2.5, 4. , 1.3],\n",
            "       [5.5, 2.6, 4.4, 1.2],\n",
            "       [6.1, 3. , 4.6, 1.4],\n",
            "       [5.8, 2.6, 4. , 1.2],\n",
            "       [5. , 2.3, 3.3, 1. ],\n",
            "       [5.6, 2.7, 4.2, 1.3],\n",
            "       [5.7, 3. , 4.2, 1.2],\n",
            "       [5.7, 2.9, 4.2, 1.3],\n",
            "       [6.2, 2.9, 4.3, 1.3],\n",
            "       [5.1, 2.5, 3. , 1.1],\n",
            "       [5.7, 2.8, 4.1, 1.3],\n",
            "       [6.3, 3.3, 6. , 2.5],\n",
            "       [5.8, 2.7, 5.1, 1.9],\n",
            "       [7.1, 3. , 5.9, 2.1],\n",
            "       [6.3, 2.9, 5.6, 1.8],\n",
            "       [6.5, 3. , 5.8, 2.2],\n",
            "       [7.6, 3. , 6.6, 2.1],\n",
            "       [4.9, 2.5, 4.5, 1.7],\n",
            "       [7.3, 2.9, 6.3, 1.8],\n",
            "       [6.7, 2.5, 5.8, 1.8],\n",
            "       [7.2, 3.6, 6.1, 2.5],\n",
            "       [6.5, 3.2, 5.1, 2. ],\n",
            "       [6.4, 2.7, 5.3, 1.9],\n",
            "       [6.8, 3. , 5.5, 2.1],\n",
            "       [5.7, 2.5, 5. , 2. ],\n",
            "       [5.8, 2.8, 5.1, 2.4],\n",
            "       [6.4, 3.2, 5.3, 2.3],\n",
            "       [6.5, 3. , 5.5, 1.8],\n",
            "       [7.7, 3.8, 6.7, 2.2],\n",
            "       [7.7, 2.6, 6.9, 2.3],\n",
            "       [6. , 2.2, 5. , 1.5],\n",
            "       [6.9, 3.2, 5.7, 2.3],\n",
            "       [5.6, 2.8, 4.9, 2. ],\n",
            "       [7.7, 2.8, 6.7, 2. ],\n",
            "       [6.3, 2.7, 4.9, 1.8],\n",
            "       [6.7, 3.3, 5.7, 2.1],\n",
            "       [7.2, 3.2, 6. , 1.8],\n",
            "       [6.2, 2.8, 4.8, 1.8],\n",
            "       [6.1, 3. , 4.9, 1.8],\n",
            "       [6.4, 2.8, 5.6, 2.1],\n",
            "       [7.2, 3. , 5.8, 1.6],\n",
            "       [7.4, 2.8, 6.1, 1.9],\n",
            "       [7.9, 3.8, 6.4, 2. ],\n",
            "       [6.4, 2.8, 5.6, 2.2],\n",
            "       [6.3, 2.8, 5.1, 1.5],\n",
            "       [6.1, 2.6, 5.6, 1.4],\n",
            "       [7.7, 3. , 6.1, 2.3],\n",
            "       [6.3, 3.4, 5.6, 2.4],\n",
            "       [6.4, 3.1, 5.5, 1.8],\n",
            "       [6. , 3. , 4.8, 1.8],\n",
            "       [6.9, 3.1, 5.4, 2.1],\n",
            "       [6.7, 3.1, 5.6, 2.4],\n",
            "       [6.9, 3.1, 5.1, 2.3],\n",
            "       [5.8, 2.7, 5.1, 1.9],\n",
            "       [6.8, 3.2, 5.9, 2.3],\n",
            "       [6.7, 3.3, 5.7, 2.5],\n",
            "       [6.7, 3. , 5.2, 2.3],\n",
            "       [6.3, 2.5, 5. , 1.9],\n",
            "       [6.5, 3. , 5.2, 2. ],\n",
            "       [6.2, 3.4, 5.4, 2.3],\n",
            "       [5.9, 3. , 5.1, 1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'frame': None, 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'), 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 150 (50 in each of three classes)\\n:Number of Attributes: 4 numeric, predictive attributes and the class\\n:Attribute Information:\\n    - sepal length in cm\\n    - sepal width in cm\\n    - petal length in cm\\n    - petal width in cm\\n    - class:\\n            - Iris-Setosa\\n            - Iris-Versicolour\\n            - Iris-Virginica\\n\\n:Summary Statistics:\\n\\n============== ==== ==== ======= ===== ====================\\n                Min  Max   Mean    SD   Class Correlation\\n============== ==== ==== ======= ===== ====================\\nsepal length:   4.3  7.9   5.84   0.83    0.7826\\nsepal width:    2.0  4.4   3.05   0.43   -0.4194\\npetal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\npetal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n============== ==== ==== ======= ===== ====================\\n\\n:Missing Attribute Values: None\\n:Class Distribution: 33.3% for each of 3 classes.\\n:Creator: R.A. Fisher\\n:Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n:Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n|details-start|\\n**References**\\n|details-split|\\n\\n- Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n  Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n  Mathematical Statistics\" (John Wiley, NY, 1950).\\n- Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n  (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n- Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n  Structure and Classification Rule for Recognition in Partially Exposed\\n  Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n  Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n- Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n  on Information Theory, May 1972, 431-433.\\n- See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n  conceptual clustering system finds 3 classes in the data.\\n- Many, many more ...\\n\\n|details-end|\\n', 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], 'filename': 'iris.csv', 'data_module': 'sklearn.datasets.data'}\n"
          ]
        }
      ],
      "source": [
        "#Write your code here\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = load_iris()\n",
        "data.target[]\n",
        "tree = DecisionTreeClassifier(random_state=99)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnZllgi5Ldel"
      },
      "source": [
        "## Question 8: Regression Decision Tree\n",
        "\n",
        "The objective of this question is to implement a Binary Classification Decision Tree with visualizations. Follow the given instructions below:\n",
        "\n",
        "#### Step 1: Import Required Libraries\n",
        "\n",
        "#### Step 2: Load and Preprocess the Data\n",
        "  - Use this data: https://raw.githubusercontent.com/KrishnaTejaJ/datasets-CSCI-B455/main/assignment6/data-rdt.csv\n",
        "\n",
        "#### Step 3: Split the Data:\n",
        "   - The data is split into training and testing sets, with 20% reserved for testing. The random state is set to 99.\n",
        "\n",
        "#### Step 4: Train the Decision Tree Regressor:\n",
        "   - A `DecisionTreeRegressor` is instantiated with a maximum depth and then trained on the training data. Choose a max_depth value that will generate the same plot as below. The range of max_depth values that you could experiment is from 1 to 5.\n",
        "\n",
        "   ![plot3](https://raw.githubusercontent.com/KrishnaTejaJ/datasets-CSCI-B455/main/imagedata/a6q10.png)\n",
        "\n",
        "#### Step 5: Evaluate the Model:\n",
        "   - The model makes predictions on the test set, and the Mean Squared Error (MSE) is calculated to evaluate the model's accuracy. The MSE is printed to the console.\n",
        "\n",
        "#### Step 6: Visualize the Results:\n",
        "   - Replicate the plot that was shown above.\n",
        "\n",
        "\n",
        "For more information, refer to the following resources:\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
        "- https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2tYICCffyrb"
      },
      "outputs": [],
      "source": [
        "#Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5OcsDcqLgOH"
      },
      "source": [
        "## Question 9: Comparing Linear Discriminant and Logistic Discriminant\n",
        "\n",
        "The objective of this question is to implement and compare Linear Discriminant and Logistic Discriminant. Follow the given instructions below:\n",
        "\n",
        "### Step 1: Load the Dataset\n",
        "- Use this data: https://raw.githubusercontent.com/KrishnaTejaJ/datasets-CSCI-B455/main/assignment6/lin_log.csv\n",
        "\n",
        "### Step 2: Split the Data\n",
        "- Divide the dataset into two parts: one for training the models and the other for testing their performance. A common split ratio is 70% for training and 30% for testing. Use 99 as the random state value.\n",
        "\n",
        "### Step 3: Preprocess the Data\n",
        "- Perform any necessary data preprocessing steps, such as feature scaling. Standardizing the features so they have a mean of 0 and a standard deviation of 1 can help improve the performance of many machine learning algorithms.\n",
        "\n",
        "### Step 4: Train Logistic Regression with Polynomial Features\n",
        "- Enhance the Logistic Regression model by adding polynomial features, which allow the model to capture more complex relationships in the data. Use a degree of polynomial that you find suitable based on the complexity of your dataset.\n",
        "- Train the enhanced Logistic Regression model on the training dataset.\n",
        "\n",
        "### Step 5: Train Linear Discriminant Analysis (LDA)\n",
        "- Train an LDA model on the training dataset. LDA tries to find a linear combination of features that best separates two or more classes of events.\n",
        "\n",
        "### Step 6: Evaluate the Models\n",
        "- Use the testing dataset to evaluate the performance of both trained models. Calculate metrics such as accuracy to understand how well each model performs.\n",
        "\n",
        "### Step 7: Visualize Decision Boundaries\n",
        "- Plot the decision boundaries for each model to visualize how they separate the classes in the dataset. Use the ```plot_decision_boundaries``` function for plotting the decision boundaries.\n",
        "\n",
        "### Step 8: Compare and Interpret Results\n",
        "- Compare the performance of the two models based on the accuracy metrics and the visualized decision boundaries.\n",
        "\n",
        "\n",
        "For more information, refer to the following resources:\n",
        " - Chapter **10. Linear Discrimination** of **Introduction to Machine Learning, 4th Edition, The MIT Press, 2020** by Ethem Alpaydin.\n",
        " - https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
        " - https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        " - https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html\n",
        " - https://scikit-learn.org/stable/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html\n",
        " - https://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_decision_regions.html\n",
        " - https://www.introspective-mode.org/logistic-regression-or-discriminant-function-analysis/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AIx9XV9zwYTz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to plot decision boundaries\n",
        "def plot_decision_boundaries(X, y, model, title):\n",
        "    h = .02  # step size in the mesh\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.5, cmap=plt.cm.Spectral)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k', cmap=plt.cm.Spectral)\n",
        "    plt.title(title)\n",
        "\n",
        "##############################################\n",
        "#Write your code from here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzjdhx3eLE7m"
      },
      "source": [
        "### **Answer the following question with a brief reasoning.**\n",
        "After comparing the Linear Discriminant and Logistic Discriminant, which method do you think draws the most appropriate decision boundary and **why**?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqYKyZMBLlJ3"
      },
      "source": [
        "Write your answer here..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
